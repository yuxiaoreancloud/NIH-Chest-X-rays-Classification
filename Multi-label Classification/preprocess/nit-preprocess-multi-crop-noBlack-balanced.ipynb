{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import h5py\n",
    "\n",
    "df = pd.read_csv('Data_Entry_2017.csv')\n",
    "bl = pd.read_csv('blacklist.csv')\n",
    "bl_list = bl['black'].tolist()\n",
    "\n",
    "# Remove rows with unreasonable ages \n",
    "df = df.drop(df.sort_values(by='Patient Age',ascending=False).head(16).index)\n",
    "df['Patient Age'] = df['Patient Age']/df['Patient Age'].max()\n",
    "\n",
    "with open('test_list.txt', 'r') as f1:\n",
    "  x = f1.read().split()\n",
    "with open('train_val_list.txt', 'r') as f2:\n",
    "  y = f2.read().split()\n",
    "\n",
    "train = df.loc[df['Image Index'].isin(y)]\n",
    "train = train.loc[~train['Image Index'].isin(bl_list)]\n",
    "#train = shuffle(train)\n",
    "#train_files_list = train['Image Index'].tolist()\n",
    "\n",
    "test = df.loc[df['Image Index'].isin(x)]\n",
    "test = test.loc[~test['Image Index'].isin(bl_list)]\n",
    "test = shuffle(test)\n",
    "test_files_list = test['Image Index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85678, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86930, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "labels_list = ['Atelectasis','Cardiomegaly','Consolidation','Edema','Effusion',\n",
    " 'Emphysema','Fibrosis','Hernia','Infiltration','Mass','No Finding',\n",
    " 'Nodule','Pleural_Thickening','Pneumonia','Pneumothorax']\n",
    "\n",
    "fibrosis = train[train['Finding Labels'].str.contains('Fibrosis')]\n",
    "\n",
    "Ca = train[train['Finding Labels'].str.contains('Cardiomegaly')]\n",
    "Co = train[train['Finding Labels'].str.contains('Consolidation')]\n",
    "Ed = train[train['Finding Labels'].str.contains('Edema')]\n",
    "Em = train[train['Finding Labels'].str.contains('Emphysema')]\n",
    "Fi = train[train['Finding Labels'].str.contains('Fibrosis')]\n",
    "He = train[train['Finding Labels'].str.contains('Hernia')]\n",
    "PT = train[train['Finding Labels'].str.contains('Pleural_Thickening')]\n",
    "P_a = train[train['Finding Labels'].str.contains('Pneumonia')]\n",
    "NF = train.loc[train['Finding Labels'] == 'No Finding']\n",
    "\n",
    "\n",
    "rest = train[~train['Finding Labels'].str.contains('Cardiomegaly')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Consolidation')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Edema')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Emphysema')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Fibrosis')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Hernia')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Pleural_Thickening')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('Pneumonia')]\n",
    "rest = rest[~rest['Finding Labels'].str.contains('No Finding')]\n",
    "\n",
    "\n",
    "# Up-sample Minority Class\n",
    "df_Ca = resample(Ca, replace=True, n_samples=3500, random_state=123)\n",
    "df_Co = resample(Co, replace=True, n_samples=3500, random_state=123)\n",
    "df_Ed = resample(Ed, replace=True, n_samples=3500, random_state=123)\n",
    "df_Em = resample(Em, replace=True, n_samples=3500, random_state=123)\n",
    "df_Fi = resample(Fi, replace=True, n_samples=3500, random_state=123)\n",
    "df_He = resample(He, replace=True, n_samples=2500, random_state=123)\n",
    "df_PT = resample(PT, replace=True, n_samples=3500, random_state=123)\n",
    "df_P_a = resample(P_a, replace=True, n_samples=3500, random_state=123)\n",
    "\n",
    "# Down-sample Majority Class \n",
    "df_NF = resample(NF, replace=False, n_samples=35000, random_state=123)\n",
    "\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balance = pd.concat([df_Ca,df_Co,df_Ed,df_Em,df_Fi,df_He,df_NF,df_PT,df_P_a,rest])\n",
    " \n",
    "# Display new class counts\n",
    "df_balance = shuffle(df_balance)\n",
    "train_files_list = df_balance['Image Index'].tolist()\n",
    "df_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24930, 12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to split labels \n",
    "def split_labels(label):\n",
    "    return label.split('|')\n",
    "\n",
    "# Store all label lists in an array \n",
    "a = df_balance['Finding Labels'].apply(split_labels)\n",
    "b = test['Finding Labels'].apply(split_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Effusion'\n",
      " 'Emphysema' 'Fibrosis' 'Hernia' 'Infiltration' 'Mass' 'No Finding'\n",
      " 'Nodule' 'Pleural_Thickening' 'Pneumonia' 'Pneumothorax']\n"
     ]
    }
   ],
   "source": [
    "# Create MultiLabelBinarizer object\n",
    "one_hot = MultiLabelBinarizer()\n",
    "\n",
    "# One-hot encode data\n",
    "trainL15 = one_hot.fit_transform(np.array(a))\n",
    "testL15 = one_hot.fit_transform(np.array(b))\n",
    "\n",
    "# List of labels corresponds to one-hot encode data above\n",
    "labels_list = one_hot.classes_\n",
    "print(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "# reshape train label to 14 labelling style\n",
    "ret = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "for i in range(trainL15.shape[0]):\n",
    "    if(trainL15[i][10]==1):\n",
    "        ret = np.vstack((ret,zero))\n",
    "    else:\n",
    "        ret = np.vstack((ret,np.delete(trainL15[i],10)))\n",
    "trainL = np.delete(ret,(0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86930"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open('./efs/crop300balanced/trainL.dat', 'wb'), trainL, allow_pickle=False)\n",
    "np.save(open('./efs/crop300balanced/train_files_list.dat', 'wb'), train_files_list, allow_pickle=False)\n",
    "#np.save(open('./efs/crop300balanced/test_files_list.dat', 'wb'), test_files_list, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import boto3\n",
    "import tempfile\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Helper method to convert images to training tensors \n",
    "def path_to_tensor(img_path, shape):\n",
    "    s3 = boto3.resource('s3', region_name='us-east-1', \n",
    "                        aws_access_key_id = 'AKIAJR75PXKNLAFCI3UQ',\n",
    "                        aws_secret_access_key= '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')\n",
    "    bucket = s3.Bucket('nih-chest-xrays-dataset')\n",
    "    object = bucket.Object('images/' + img_path )\n",
    "    \n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    tmp = tempfile.NamedTemporaryFile()\n",
    "    \n",
    "    with open(tmp.name, 'wb') as f:\n",
    "        object.download_fileobj(f)\n",
    "        # loads RGB image as PIL.Image.Image type\n",
    "        img = image.load_img(tmp.name, target_size=shape)\n",
    "        # crop image to 224*224 if shape is 300*300\n",
    "        cropped_im = img.crop((50,50,350,350))\n",
    "        #imshow(np.asarray(img))\n",
    "        x = image.img_to_array(cropped_im)/255\n",
    "        # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "        return np.expand_dims(x, axis=0)\n",
    "\n",
    "# Convert images to training tensors \n",
    "def paths_to_tensor(img_paths, shape):\n",
    "    list_of_tensors = [path_to_tensor(img_path, shape) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3ec93784174a93b5a179b2afd75be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split training tensors (images)\n",
    "img_shape = (400,400)\n",
    "train_tensors = paths_to_tensor(train_files_list[:76000], shape = img_shape)\n",
    "\n",
    "# Save training tensors (images)\n",
    "with h5py.File('./efs/crop300balanced/training.hdf5', 'w') as hf:\n",
    "    hf.create_dataset(\"training\",  data=train_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cef26d264e941ac8993ff2ecc9da70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10930), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_shape = (400,400)\n",
    "valid_tensors = paths_to_tensor(train_files_list[76000:], shape = img_shape)\n",
    "np.save(open('./efs/crop300balanced/validation.dat', 'wb'), valid_tensors, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training labels \n",
    "train_labels = trainL[:76000]\n",
    "valid_labels = trainL[76000:]\n",
    "test_labels = testL[:]\n",
    "# Save training labels \n",
    "np.save(open('./efs/crop300balanced/trainLabels.dat', 'wb'), train_labels, allow_pickle=False)\n",
    "np.save(open('./efs/crop300balanced/validLabels.dat', 'wb'), valid_labels, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with h5py.File('./efs/training.hdf5', 'r') as hf:\n",
    "    train_tensors = hf['./efs/training'][:]\n",
    "valid_tensors = np.load('./efs/validation.dat')\n",
    "test_tensors = np.load('./efs/testing.dat')\n",
    "\n",
    "train_labels = np.load('./efs/trainLabels.dat')\n",
    "valid_labels = np.load('./efs/validLabels.dat')\n",
    "test_labels = np.load('./efs/testLabels.dat')\n",
    "\n",
    "train_data = np.load('./efs/trainData.dat')\n",
    "valid_data = np.load('./efs/validData.dat')\n",
    "test_data = np.load('./efs/testData.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 10.8MB/s ta 0:00:01\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.24.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/7d/b1dedde8af99bd82f20ed7e9697aac0597de3049b1f786aa2aac3b9bd4da/Keras-2.2.2-py2.py3-none-any.whl (299kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 15.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Collecting keras-preprocessing==1.0.2 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.14.5)\n",
      "Collecting keras-applications==1.0.4 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 31.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.2.2 keras-applications-1.0.4 keras-preprocessing-1.0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/95/274190b39950e1e9eef4b071acefea832ac3e2c19bb4b442fa54f3214d2e/tensorflow-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (51.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 51.1MB 1.0MB/s eta 0:00:01    79% |█████████████████████████▍      | 40.6MB 58.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools<=39.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.14.5)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/f6/b1a6a703620ac4c393d286b0289c6bb51294629aa1cae8ef3bc1bcafd164/grpcio-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (9.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.3MB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Collecting tensorboard<1.10.0,>=1.9.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 17.3MB/s ta 0:00:01    16% |█████▍                          | 552kB 68.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.5.2)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/5d/18feb90462c8edaae71305716c7e8bac479fc9dface63221f808a6b95880/absl-py-0.3.0.tar.gz (84kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 41.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.10.0,>=1.9.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 37.3MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, gast, absl-py\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/4c/16/ef/e36a23f2432e9220f8845f94e2c3abd39e7d9d1cd458d3159d\n",
      "Successfully built termcolor gast absl-py\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: grpcio, termcolor, gast, markdown, tensorboard, astor, absl-py, tensorflow\n",
      "Successfully installed absl-py-0.3.0 astor-0.7.1 gast-0.2.0 grpcio-1.14.0 markdown-2.6.11 tensorboard-1.9.0 tensorflow-1.9.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
